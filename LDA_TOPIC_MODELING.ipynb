{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1LasZaclYLU7KWxr3zfmFElsGEmdfG2NX","timestamp":1709241547915}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GOAL:\n","The goal of this notebook is to perform LDA topic modeling using the training data, and then applying the results on the test set. This will complete the dataset construction part of the pipeline, allowing us to move on to the models."],"metadata":{"id":"EapEOkvGGVmT"}},{"cell_type":"code","source":["from google.colab import drive\n","import sys\n","\n","drive.mount('/content/drive')\n","sys.path.append('/content/drive/My Drive/Courses/CS247/247 Project')\n","%cd /content/drive/My\\ Drive/Courses/CS247/247 Project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mClW0oMZJBhq","executionInfo":{"status":"ok","timestamp":1710199304428,"user_tz":420,"elapsed":25644,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"6baf5c0a-d827-488e-cd6a-c12fd8ba6f6e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Courses/CS247/247 Project\n"]}]},{"cell_type":"code","source":["!pip3 install emoji==0.6.0\n","!pip install --upgrade gensim\n","!pip install pyLDAvis\n","!pip install tomotopy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-3aCp_R2c_J","executionInfo":{"status":"ok","timestamp":1710199352136,"user_tz":420,"elapsed":47711,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"a31f255b-22ea-4a19-be49-54189255b7bb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting emoji==0.6.0\n","  Downloading emoji-0.6.0.tar.gz (51 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/51.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49719 sha256=a5d053e7f0ce42ea9e1190cd1059da87a9a8f714747ae695b04941508757c500\n","  Stored in directory: /root/.cache/pip/wheels/1b/bd/d9/310c33c45a553798a714e27e3b8395d37128425442b8c78e07\n","Successfully built emoji\n","Installing collected packages: emoji\n","Successfully installed emoji-0.6.0\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n","Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n","Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n","Collecting pyLDAvis\n","  Downloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.24.2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.11.4)\n","Collecting pandas>=2.0.0 (from pyLDAvis)\n","  Downloading pandas-2.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (3.1.3)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (2.9.0)\n","Collecting funcy (from pyLDAvis)\n","  Downloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n","Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (1.2.2)\n","Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (4.3.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pyLDAvis) (67.7.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=2.0.0->pyLDAvis) (2023.4)\n","Collecting tzdata>=2022.7 (from pandas>=2.0.0->pyLDAvis)\n","  Downloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.4/345.4 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.3.0)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim->pyLDAvis) (6.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->pyLDAvis) (2.1.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.16.0)\n","Installing collected packages: funcy, tzdata, pandas, pyLDAvis\n","  Attempting uninstall: pandas\n","    Found existing installation: pandas 1.5.3\n","    Uninstalling pandas-1.5.3:\n","      Successfully uninstalled pandas-1.5.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","bigframes 0.23.0 requires pandas<2.1.4,>=1.5.0, but you have pandas 2.2.1 which is incompatible.\n","google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.2.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed funcy-2.0 pandas-2.2.1 pyLDAvis-3.4.1 tzdata-2024.1\n","Collecting tomotopy\n","  Downloading tomotopy-0.12.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.2/17.2 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tomotopy) (1.25.2)\n","Installing collected packages: tomotopy\n","Successfully installed tomotopy-0.12.7\n"]}]},{"cell_type":"code","source":["import nltk\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","nltk.download('stopwords')\n","import pandas as pd\n","\n","import gensim\n","from gensim.corpora import Dictionary\n","from gensim.corpora import MmCorpus\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cTdF0BM_3XmI","executionInfo":{"status":"ok","timestamp":1710199356544,"user_tz":420,"elapsed":4411,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"4fe17633-b58c-47c3-e6de-ba3b95deba64"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"markdown","source":["## Data Processing"],"metadata":{"id":"vLzwxd9Owk4Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"kCaDP5_AEboJ","executionInfo":{"status":"ok","timestamp":1710199357878,"user_tz":420,"elapsed":1336,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"479f6dad-f127-4632-cec9-f21d7b8dd6b6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['action', 'litw', 'dynahate', 'perspective', 'rewire', 'hateroberta',\n","       'gpt4', 'gender', 'ethnicity', 'annotator_id'],\n","      dtype='object')"]},"metadata":{},"execution_count":4}],"source":["import pandas as pd\n","\n","data = pd.read_csv('./data/nlpositionality_toxicity_processed.csv')\n","data['annotator_id'] = range(len(data))\n","data = data.drop(['session_id', 'age', 'religion', 'education', 'country_longest', 'country_residence', 'native_language'], axis=1)\n","data.to_csv('./data/toxicity_processed.csv', index=False)\n","data.columns"]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","train_df, test_df = train_test_split(data, test_size=0.2, random_state=8)\n","train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=8)\n","\n","\n","train_df.to_csv('./data/toxicity_processed_train.csv', index=False)\n","val_df.to_csv('./data/toxicity_processed_val.csv', index=False)\n","test_df.to_csv('./data/toxicity_processed_test.csv', index=False)\n","\n","\n","print(\"train: \", len(train_df))\n","print(\"val: \", len(val_df))\n","print(\"test: \", len(test_df))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5GUgAcgoc51","executionInfo":{"status":"ok","timestamp":1710199359299,"user_tz":420,"elapsed":1422,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"179380fc-d5aa-4a10-9849-d18163642972"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["train:  5107\n","val:  568\n","test:  1419\n"]}]},{"cell_type":"code","source":["tweet_text = data['action'].tolist()\n","humman_label = data['litw'].tolist()"],"metadata":{"id":"aLsNabK51TiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import RegexpTokenizer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","from nltk.corpus import stopwords\n","\n","def tokenization(tweet_list):\n","  tweet_corpus_tokenized = []\n","  tokenizer = RegexpTokenizer(r'\\w+')\n","  for tweet in tweet_list:\n","    tokenized_lyric = tokenizer.tokenize(tweet.lower())\n","    tweet_corpus_tokenized.append(tokenized_lyric)\n","  return tweet_corpus_tokenized\n","\n","def token_filtering(tweet_tokenized_list):\n","  for idx, tweet in enumerate(tweet_tokenized_list):\n","    filtered_tweets = []\n","    for token in tweet:\n","        if len(token) > 2 and not token.isnumeric():\n","            filtered_tweets.append(token)\n","    tweet_tokenized_list[idx] = filtered_tweets\n","  return tweet_tokenized_list\n","\n","def lemmatization(tweet_corpus_tokenized):\n","  lemmatizer = WordNetLemmatizer()\n","  for idx, tweet in enumerate(tweet_corpus_tokenized):\n","    lemmatized_tokens = []\n","    for token in tweet:\n","        lemmatized_tokens.append(lemmatizer.lemmatize(token))\n","    tweet_corpus_tokenized[idx] = lemmatized_tokens\n","  return tweet_corpus_tokenized\n","\n","def remove_stop_words(tweet_corpus_tokenized):\n","  stop_words = stopwords.words('english')\n","  for idx, tweet in enumerate(tweet_corpus_tokenized):\n","    filtered_text = []\n","    for token in tweet:\n","        if not contains_non_ascii(token) and token not in stop_words:\n","            filtered_text.append(token)\n","    tweet_corpus_tokenized[idx] = filtered_text\n","  return tweet_corpus_tokenized\n","\n","def contains_non_ascii(word):\n","    for char in word:\n","        if ord(char) >= 128:\n","            return True\n","    return False\n"],"metadata":{"id":"nDoVtLQWJ19J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet_tokenized = tokenization(tweet_text)\n","tweet_tokenized = token_filtering(tweet_tokenized)\n","tweet_tokenized = lemmatization(tweet_tokenized)\n","tweet_tokenized = remove_stop_words(tweet_tokenized)"],"metadata":{"id":"Loi3kTTswTbV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#LDA Modeling"],"metadata":{"id":"yrx0YIPLwtq8"}},{"cell_type":"markdown","source":["##Gensim"],"metadata":{"id":"c--ZUp9G8VyX"}},{"cell_type":"code","source":["import gensim\n","from gensim.corpora import Dictionary\n","from gensim.corpora import MmCorpus\n","\n","dictionary = Dictionary(tweet_tokenized)\n","dictionary.filter_extremes(no_below=100, no_above=0.8)\n","\n","gensim_corpus = [dictionary.doc2bow(tweet) for tweet in tweet_tokenized]\n","temp = dictionary[0]\n","id2word = dictionary.id2token"],"metadata":{"id":"aVGdOYjYwvZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim\n","from gensim import models, test\n","#from gensim.models import LDAModel\n","from gensim.test import utils\n","from gensim.test.utils import datapath\n","\n","lda_model = models.LdaModel(\n","    corpus=gensim_corpus,\n","    id2word=id2word,\n","    chunksize=2000,\n","    alpha='auto',\n","    eta='auto',\n","    iterations=400,\n","    num_topics=6,\n","    passes=20\n",")"],"metadata":{"id":"-eSeLu18x_aI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_model_save = datapath(\"/content/drive/My Drive/Courses/CS247/247 Project/models/lda-6\")\n","lda_model.save(train_model_save)"],"metadata":{"id":"jAjKYRaI6gjM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for idx, topic in lda_model.show_topics(formatted=False, num_words=15):\n","    print('Topic: {} \\nWords: {}'.format(idx, '|'.join([w[0] for w in topic])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sS2419_73jQF","executionInfo":{"status":"ok","timestamp":1710199400727,"user_tz":420,"elapsed":4,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"32f21c97-5ba7-480a-d0b6-d25965214801"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Topic: 0 \n","Words: woman|jew|one|british|believe|back|much|gay|wa|nothing|men|man|world|european|rape\n","Topic: 1 \n","Words: always|woman|men|idiot|never|take|willing|victim|someone|wear|le|clothing|today|fault|else\n","Topic: 2 \n","Words: wa|like|people|would|want|think|shit|see|anyone|get|got|around|make|could|else\n","Topic: 3 \n","Words: people|like|get|muslim|fucking|country|white|immigrant|guy|right|make|time|black|actually|work\n","Topic: 4 \n","Words: white|people|know|need|great|really|western|dangerous|say|get|word|animal|meme|way|group\n","Topic: 5 \n","Words: people|issue|race|also|least|history|poverty|suffering|perceived|land|infamous|brutality|descended|african|africa\n"]}]},{"cell_type":"markdown","source":["#Tomotopy\n","https://github.com/bab2min/tomotopy\n","https://bab2min.github.io/tomotopy/v0.12.6/en/"],"metadata":{"id":"DpeKk_7r8ZjA"}},{"cell_type":"markdown","source":["### LDA"],"metadata":{"id":"Lzv4yifPibtg"}},{"cell_type":"code","source":["import tomotopy as tp\n","import random\n","\n","seed = int(random.randint(1, 300000))\n","\n","mdl = tp.LDAModel(k=10, seed=seed)\n","for each in tweet_tokenized:\n","  mdl.add_doc(each)\n","\n","for i in range(0, 100, 10):\n","    mdl.train(10)\n","    print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n","\n","for k in range(mdl.k):\n","    print('Top 10 words of topic #{}'.format(k))\n","    print(mdl.get_topic_words(k, top_n=10))\n","\n","mdl.summary()\n","mdl.save('./models/tomotopy-lda-{}-{}.bin'.format(mdl.k, seed), True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KaqHoNNH8SqE","executionInfo":{"status":"ok","timestamp":1710199461223,"user_tz":420,"elapsed":19006,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"5c47f6a4-2387-4077-c967-cec48bf8b373"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-14-6239f2e78041>:11: RuntimeWarning: The training result may differ even with fixed seed if `workers` != 1.\n","  mdl.train(10)\n"]},{"output_type":"stream","name":"stdout","text":["Iteration: 0\tLog-likelihood: -6.2816301213533565\n","Iteration: 10\tLog-likelihood: -6.060579213647868\n","Iteration: 20\tLog-likelihood: -6.002724317650043\n","Iteration: 30\tLog-likelihood: -5.974921130481547\n","Iteration: 40\tLog-likelihood: -5.957357149571673\n","Iteration: 50\tLog-likelihood: -5.950974164825691\n","Iteration: 60\tLog-likelihood: -5.9438444192035504\n","Iteration: 70\tLog-likelihood: -5.936114216250063\n","Iteration: 80\tLog-likelihood: -5.932787849036342\n","Iteration: 90\tLog-likelihood: -5.928497375949288\n","Top 10 words of topic #0\n","[('fuck', 0.01643390581011772), ('get', 0.01465738657861948), ('people', 0.013658096082508564), ('old', 0.011659513227641582), ('white', 0.010993318632245064), ('make', 0.010993318632245064), ('like', 0.01054918859153986), ('planet', 0.009660929441452026), ('ape', 0.009438864886760712), ('jew', 0.008994734846055508)]\n","Top 10 words of topic #1\n","[('people', 0.021816875785589218), ('know', 0.01999225653707981), ('muslim', 0.017374327406287193), ('non', 0.013725091703236103), ('thankfully', 0.012297130189836025), ('europe', 0.012217799201607704), ('excuse', 0.01142448652535677), ('gay', 0.010155186988413334), ('see', 0.009282544255256653), ('become', 0.009282544255256653)]\n","Top 10 words of topic #2\n","[('like', 0.01653299480676651), ('would', 0.014816547743976116), ('white', 0.013551797717809677), ('black', 0.013190439902245998), ('dwarf', 0.012738743796944618), ('life', 0.01246772613376379), ('guy', 0.011745011433959007), ('shit', 0.009396190755069256), ('islam', 0.009215512312948704), ('act', 0.009034832939505577)]\n","Top 10 words of topic #3\n","[('history', 0.01820228062570095), ('people', 0.017347753047943115), ('without', 0.015980510041117668), ('man', 0.01410054974257946), ('black', 0.013844192028045654), ('believe', 0.013416928239166737), ('least', 0.013331475667655468), ('race', 0.013331475667655468), ('land', 0.013331475667655468), ('exception', 0.0132460230961442)]\n","Top 10 words of topic #4\n","[('people', 0.034482527524232864), ('like', 0.016940487548708916), ('white', 0.015961747616529465), ('woman', 0.014907719567418098), ('race', 0.013627828098833561), ('would', 0.013025526888668537), ('also', 0.012950238771736622), ('fuck', 0.012197362259030342), ('say', 0.011218621395528316), ('hating', 0.011218621395528316)]\n","Top 10 words of topic #5\n","[('retard', 0.014097487553954124), ('make', 0.014097487553954124), ('back', 0.011777794919908047), ('people', 0.010171853937208652), ('immigrant', 0.010171853937208652), ('everyone', 0.00972575880587101), ('different', 0.009458102285861969), ('really', 0.009458102285861969), ('jew', 0.008298255503177643), ('change', 0.007584503851830959)]\n","Top 10 words of topic #6\n","[('woman', 0.03951923921704292), ('always', 0.03554850071668625), ('never', 0.020800046622753143), ('take', 0.020516421645879745), ('idiot', 0.017680181190371513), ('men', 0.01749109849333763), ('someone', 0.01701839081943035), ('wear', 0.01635660231113434), ('else', 0.01635660231113434), ('today', 0.016262060031294823)]\n","Top 10 words of topic #7\n","[('wa', 0.018227949738502502), ('people', 0.016191957518458366), ('ha', 0.012895587831735611), ('time', 0.011053500697016716), ('started', 0.01085959654301405), ('fucking', 0.01027788408100605), ('rape', 0.010083979927003384), ('even', 0.009987028315663338), ('waste', 0.009308364242315292), ('retard', 0.009017507545650005)]\n","Top 10 words of topic #8\n","[('people', 0.026144322007894516), ('wa', 0.02083517797291279), ('really', 0.0159282386302948), ('white', 0.015526031143963337), ('million', 0.015204264782369137), ('nation', 0.014319407753646374), ('word', 0.012791017070412636), ('get', 0.012710575945675373), ('great', 0.012388809584081173), ('way', 0.012227926403284073)]\n","Top 10 words of topic #9\n","[('woman', 0.02365260012447834), ('men', 0.018816744908690453), ('get', 0.01503598690032959), ('people', 0.012310324236750603), ('like', 0.011167304590344429), ('place', 0.009057113900780678), ('man', 0.008881265297532082), ('hard', 0.008617491461336613), ('muslim', 0.008002018555998802), ('anti', 0.007826169952750206)]\n","<Basic Info>\n","| LDAModel (current version: 0.12.7)\n","| 7094 docs, 113347 words\n","| Total Vocabs: 2238, Used Vocabs: 2238\n","| Entropy of words: 7.07497\n","| Entropy of term-weighted words: 7.07497\n","| Removed Vocabs: <NA>\n","|\n","<Training Info>\n","| Iterations: 100, Burn-in steps: 0\n","| Optimization Interval: 10\n","| Log-likelihood per word: -5.92850\n","|\n","<Initial Parameters>\n","| tw: TermWeight.ONE\n","| min_cf: 0 (minimum collection frequency of words)\n","| min_df: 0 (minimum document frequency of words)\n","| rm_top: 0 (the number of top words to be removed)\n","| k: 10 (the number of topics between 1 ~ 32767)\n","| alpha: [0.1] (hyperparameter of Dirichlet distribution for document-topic, given as a single `float` in case of symmetric prior and as a list with length `k` of `float` in case of asymmetric prior.)\n","| eta: 0.01 (hyperparameter of Dirichlet distribution for topic-word)\n","| seed: 160469 (random seed)\n","| trained in version 0.12.7\n","|\n","<Parameters>\n","| alpha (Dirichlet prior on the per-document topic distributions)\n","|  [0.00560164 0.00528379 0.00471007 0.00551564 0.00661509 0.00556425\n","|   0.00534637 0.00441777 0.00447127 0.00483816]\n","| eta (Dirichlet prior on the per-topic word distribution)\n","|  0.01\n","|\n","<Topics>\n","| #0 (8984) : fuck get people old white\n","| #1 (12583) : people know muslim non thankfully\n","| #2 (11047) : like would white black dwarf\n","| #3 (11680) : history people without man black\n","| #4 (13260) : people like white woman race\n","| #5 (11186) : retard make back people immigrant\n","| #6 (10555) : woman always never take idiot\n","| #7 (10292) : wa people ha time started\n","| #8 (12409) : people wa really white million\n","| #9 (11351) : woman men get people like\n","|\n","\n"]}]},{"cell_type":"markdown","source":["### Hierarchical LDA"],"metadata":{"id":"otxwgmEnidso"}},{"cell_type":"code","source":["# mhdl = tp.HLDAModel(k=10)\n","# for each in tweet_tokenized:\n","#   mhdl.add_doc(each)\n","\n","# # for i in range(0, 100, 10):\n","# #     mdl.train(10)\n","# #     print('Iteration: {}\\tLog-likelihood: {}'.format(i, mdl.ll_per_word))\n","\n","# # for k in range(mdl.k):\n","# #     print('Top 10 words of topic #{}'.format(k))\n","# #     print(mdl.get_topic_words(k, top_n=10))\n","\n","# mhdl.summary()\n","# mhdl.save('./models/tomotopy-hlda.bin', True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"8C5v7XDkijYX","executionInfo":{"status":"error","timestamp":1709272462679,"user_tz":480,"elapsed":168,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"e9bec8f5-82cb-487a-c789-6171eaae8c13"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'k' is an invalid keyword argument for this function","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-0eac98de2213>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmhdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHLDAModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweet_tokenized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mmhdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for i in range(0, 100, 10):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'k' is an invalid keyword argument for this function"]}]},{"cell_type":"markdown","source":["# NEXT: Write New Dataset Files w/ LDA Labels\n","This will assign a topic to each artist, lyric pair in the train and test sets. Later, we will rewrite the dataset into a csv file."],"metadata":{"id":"_cN5ZTOM1lrV"}},{"cell_type":"code","source":["Y_test_tokenized = lyric_tokenization(Y_test)\n","Y_test_tokenized = token_filtering(Y_test_tokenized)\n","Y_test_tokenized = lemmatization(Y_test_tokenized)\n","Y_test_tokenized = remove_stop_words(Y_test_tokenized)"],"metadata":{"id":"_GoOHzzG1oQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# write the training csv\n","with open('/content/drive/Shareddrives/CS260-Project/data/lda-train-6-updated.csv', 'w') as traindata:\n","  writer = csv.writer(traindata, delimiter=',')\n","  writer.writerow(['artist', 'topic_id', 'lyric'])\n","  line = 0\n","  for i, artist in enumerate(X_train):\n","    curr_doc = dictionary.doc2bow(Y_train_tokenized[i])\n","    #gensim_corpus = [dictionary.doc2bow(song) for song in Y_train_tokenized]\n","    probs = lda_model[curr_doc]\n","    max_prob = -1\n","    topic_id = -1\n","    for j in range(len(probs)):\n","      idx, curr_prob = probs[j]\n","      if curr_prob > max_prob:\n","        max_prob = curr_prob\n","        topic_id = idx\n","    if line < 4:\n","      print(probs)\n","      print(Y_train[i])\n","      print(topic_id)\n","    #max_prob = max(probs)\n","    #topic_id = probs.index(max_prob)\n","    lyrics = Y_train[i]\n","    writer.writerow([artist, topic_id, lyrics])\n","    line += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJcxQosz9W1y","executionInfo":{"status":"ok","timestamp":1669751322503,"user_tz":480,"elapsed":88915,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"27f6eb02-0d8f-4219-acb3-346c38139770"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[(0, 0.25901562), (2, 0.03261063), (3, 0.43661842), (4, 0.23224649), (5, 0.034634035)]\n","Bee Gees - When A Lonely Heart Breaks\n","\n","\n","I stumble in the night\n","Never really knew what it would've been like\n","You're no longer there to break my fall\n","The heartache over you\n","I'd give it everything but I couldn't live through\n","I never saw the signs\n","You're the last to know when love is blind.\n","\n","All the tears and the turbulent years\n","When I would not wait for no-one\n","Didn't stop and take a look at myself\n","And see me losing you.\n","\n","(Chorus)\n","When a lonely heart breaks\n","It's the one that forsakes\n","It's the dream that we stole\n","And I'm missing you more\n","Than the fire that will roar\n","There's a hole in my soul\n","For you it's good-bye\n","For me it's to cry\n","For whom the bell tolls.\n","\n","Seen you in a magazine\n","A picture at a party where you shouldn't have been\n","Hanging on the arm of someone else\n","I'm still in love with you\n","Won't you come back to your little boy blue\n","I've come to feel inside\n","This precious love was never mine.\n","\n","Now I know but a little too late\n","That I could not live without you\n","In the dark or the broad daylight\n","I promise I'll be there.\n","\n","(Chorus)\n","When a lonely heart breaks\n","It's the one that forsakes\n","It's the dream that we stole\n","And I'm missing you more\n","Than the fire that will roar\n","There's a hole in my soul\n","For you it's good-bye\n","For me it's to cry\n","For whom the bell tolls.\n","\n","Now I know there'll be times like this\n","When I couldn't reach out to no-one\n","Am I never gonna find someone\n","Who knows me like you do\n","Are you leaving me a helpless child\n","When it took so long to save me\n","Fight the devil and the deep blue sea\n","I'll follow you anywhere\n","I promise I'll be there.\n","\n","(Chorus)\n","When a lonely heart breaks\n","It's the one that forsakes\n","It's the dream that we stole\n","And I'm missing you more\n","Than the fire that will roar\n","There's a hole in my soul\n","For you it's good-bye\n","For me it's to cry\n","For whom the bell tolls.\n","3\n","[(0, 0.55768764), (1, 0.08613628), (2, 0.028652389), (3, 0.29223907), (4, 0.03184618)]\n","She was the queen of evil\n","Ridin' on the storm\n","With the heart of the devil\n","Will we ever learn\n","\n","She called the rolling thunder\n","Cold wind and rain\n","Tearin' waves from down under\n","They still can feel the pain\n","\n","Still a long way to go\n","They were lost in the zone\n","Dreamin' of a holy wonder\n","No time to return\n","\n","They're ridin' to the tower\n","On the ship of the lost\n","Storm made of power\n","And the hull turned to dust\n","\n","Sailing away\n","Into the fire\n","Sailing away\n","Lost children of the sea\n","\n","\n","0\n","[(0, 0.017308677), (2, 0.013279248), (3, 0.31832522), (4, 0.6421694)]\n","She just started liking cheatin' songs and what's bothering me\n","I don't know if it's the cheatin' she likes or just the melodies\n","\n","I've never had any reason to doubt her as far as I know she's been true\n","But lately she's had a different look in her eye I wonder if she's seeing somebody new\n","\n","You can't blame a body for looking when there's whole world of people out there\n","But I hope she's not taking any closer looks I tell you lately frankly I've been scared\n","\n","Cause she just started liking...\n","[ fiddle - guitar ]\n","I've got to admit I do some looking too but she's the only one I love\n","She still tells me she still loves me but I wonder if my love's enough\n","\n","Cause she just started liking...\n","Yes she just started liking...\n","4\n","[(1, 0.54978955), (4, 0.42529443)]\n","[Ester Dean]\n","Here we go ready hard rock\n","Here we go ready hard rock\n","(x3)\n","\n","[50 cent]\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout (touch my ass)\n","(x3)\n","\n","[Ester Dean]\n","You can touch wherever you want\n","please don’t touch my ass\n","\n","[50 cent]\n","I like the way that you pop it\n","you know you turn me on\n","i know the way that you move it you really got me going\n","girl you make … say nothing the way your body talk\n","me I’m just patiently waiting to see you take it off\n","\n","Shawty not complicated, I mean don’t complicate it\n","I wanna get you off the floor, you and i be fornicating\n","thats how it’s supposed to go\n","I know you know just how to drop it low\n","drop drop it low, drop it low\n","dr dr drop it low\n","\n","[Ester Dean]\n","All my ladies let me see you throw your legs up\n","put your hands up ladies put your hands up\n","and all the fellas grab my ta ta’s\n","fellas grab your mama\n","you can touch wherever you want\n","please don’t touch my ass\n","\n","[50 cent]\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout (touch my ass)\n","(x3)\n","\n","[Ester Dean]\n","you can touch wherever you want\n","please don’t touch my ass\n","\n","[50 cent]\n","only a hood girl no good girl\n","maybe a superfreak\n","i dont wanna talk girl, i wanna fuck girl\n","then maybe we can grab a bite to eat\n","if you wanna play games we can play games\n","for you can play with me all day\n","we dont have to wait girl we can mate girl\n","we can get it in right away\n","funny how you like it\n","i’ll give it to you every way you want\n","not like a cow girl, you hesitating baby get up on it\n","off that shy shit\n","off them good girl games\n","off now all clothes\n","off when you walk with me\n","\n","[Ester Dean]\n","All my ladies let me see you throw your legs up\n","put your hands up ladies put your hands up\n","and all the fellas grab my ta ta’s\n","fellas grab your mama\n","you can touch wherever you want\n","please don’t touch my ass\n","\n","[50 cent]\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout (touch my ass)\n","(x3)\n","\n","[Ester Dean]\n","You can touch wherever you want\n","please don’t touch my ass\n","\n","I’ll scream make a scene if you touch that\n","thats right I’ll fight if you rub that\n","i know its all about it\n","i know its all about it\n","girls stop playing with me, with me\n","\n","[50 cent]\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout (touch my ass)\n","(x3)\n","\n","[Ester Dean]\n","You can touch wherever you want\n","please don’t touch my ass\n","\n","All my ladies let me see you throw your legs up\n","put your hands up ladies put your hands up\n","and all the fellas grab my ta ta’s\n","fellas grab your mama\n","you can touch wherever you want\n","please don’t touch my ass\n","\n","[50 cent]\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout\n","What ya talkin bout (touch my ass)\n","(x3)\n","\n","[Ester Dean]\n","You can touch wherever you want\n","please don’t touch my ass\n","1\n"]}]},{"cell_type":"code","source":["# write the test csv\n","print(Y_test[0])\n","\n","with open('/content/drive/Shareddrives/CS260-Project/data/lda-test-6-updated.csv', 'w') as testdata:\n","  writer = csv.writer(testdata, delimiter=',')\n","  writer.writerow(['artist', 'topic_id', 'lyric'])\n","  line = 0\n","  for i, artist in enumerate(X_test):\n","    curr_doc = dictionary.doc2bow(Y_test_tokenized[i])\n","    #gensim_corpus = [dictionary.doc2bow(song) for song in Y_train_tokenized]\n","    probs = lda_model[curr_doc]\n","    max_prob = -1\n","    topic_id = -1\n","    for j in range(len(probs)):\n","      idx, curr_prob = probs[j]\n","      if curr_prob > max_prob:\n","        max_prob = curr_prob\n","        topic_id = idx\n","    if line < 4:\n","      print(probs)\n","      print(Y_test[i])\n","      print(topic_id)\n","    #max_prob = max(probs)\n","    #topic_id = probs.index(max_prob)\n","    lyrics = Y_test[i]\n","    writer.writerow([artist, topic_id, lyrics])\n","    line += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DmLn3RN4K9KR","executionInfo":{"status":"ok","timestamp":1669751361262,"user_tz":480,"elapsed":22697,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"1aa7b72d-1927-44b8-ceb9-7b127347d8a9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Your love is so good for me\n","Your love is so good for me\n","\n","Every day you're on my mind\n","Wanna be near you all the time\n","You made my poem rhyme\n","And my heart began to sing again\n","The day your eyes met mine\n","\n","Your love is so good for me\n","Your love is so good for me\n","\n","Two hearts just running free\n","Like a wind song through the trees\n","Your love does not possess\n","It just holds me where I wanna be\n","With binds of tenderness\n","\n","Your love is so good for me\n","Your love is so good for me\n","\n","Baby, you know your love is so good\n","It's good\n","You know it's good\n","\n","Like a star up in the sky\n","Burning brightly, you and I\n","Time will tell if love survives\n","For we only have today\n","And today love is alive\n","\n","Your love is so good for me\n","Your love is so good for me\n","\n","It's so good\n","You know your love is so good,\n","So good for me...\n","It's so good\n","\n","No need to say the words\n","When you touch me they will be heard\n","You gave so much to me\n","And you showed me how to love the way\n","True love was meant to be\n","\n","Your love is so good for me\n","Your love is so good for me\n","\n","[(0, 0.12346744), (3, 0.25593066), (4, 0.6060558)]\n","Your love is so good for me\n","Your love is so good for me\n","\n","Every day you're on my mind\n","Wanna be near you all the time\n","You made my poem rhyme\n","And my heart began to sing again\n","The day your eyes met mine\n","\n","Your love is so good for me\n","Your love is so good for me\n","\n","Two hearts just running free\n","Like a wind song through the trees\n","Your love does not possess\n","It just holds me where I wanna be\n","With binds of tenderness\n","\n","Your love is so good for me\n","Your love is so good for me\n","\n","Baby, you know your love is so good\n","It's good\n","You know it's good\n","\n","Like a star up in the sky\n","Burning brightly, you and I\n","Time will tell if love survives\n","For we only have today\n","And today love is alive\n","\n","Your love is so good for me\n","Your love is so good for me\n","\n","It's so good\n","You know your love is so good,\n","So good for me...\n","It's so good\n","\n","No need to say the words\n","When you touch me they will be heard\n","You gave so much to me\n","And you showed me how to love the way\n","True love was meant to be\n","\n","Your love is so good for me\n","Your love is so good for me\n","\n","4\n","[(0, 0.3616499), (2, 0.11313136), (3, 0.20954442), (4, 0.3059986)]\n","Well I see trees of green and\n","Red roses too,\n","I'll watch them bloom for me and you\n","And I think to myself\n","What a wonderful world\n","\n","Well I see skies of blue and I see clouds of white\n","And the brightness of day\n","I like the dark and I think to myself\n","What a wonderful world\n","\n","The colors of the rainbow so pretty in the sky\n","Are also on the faces of people passing by\n","I see friends shaking hands\n","Saying, \"How do you do?\"\n","They're really saying, I...I love you\n","I hear babies cry and I watch them grow,\n","They'll learn much more\n","Than we'll know\n","And I think to myself\n","What a wonderful world (w)oohoorld\n","0\n","[(0, 0.28236258), (2, 0.06820256), (3, 0.22547507), (4, 0.41995355)]\n","It's the midnight hour and I with you were here\n","Somebody following me, I just can't sleep alone\n","I figure it's the way I am ,or a thing that\n","Happened to me somewhere in my childhood\n","I'm all that I could ever be in your arms\n","It's like I can't imagine life without you\n","But all my women cry\n","And all my heroes die, or we could try a little\n","\n","Moonlight madness\n","We could do the strangest things\n","Try a little moonlight madness\n","Let someone in your dreams\n","Or we could try a little moonlight madness\n","Living on the same old lies\n","We should not try to find the meaning\n","Let history decide\n","Whether it's right or wrong to give you\n","What I may give you\n","How we learn to do the things,we never do\n","Let me try a little moonlight madness\n","On you\n","\n","I cry with a smile and I laugh with a tear\n","I struggle but the body won't move\n","And the quicksand pulls me down\n","I wanna be the man that you think I am\n","I follow in the steps of the one\n","Who knows the darkness well\n","And you can get me by your side\n","By talking to me with your eyes\n","Not breaking promises\n","I'm howling at the moon\n","I'm howling,howling at the moon\n","Or we could try a little\n","\n","Moonlight madness\n","We could do the strangest things\n","Try a little something different\n","The mystery remains\n","Whether it's right or wrong to love each other\n","Yearning in the brain for something new\n","Let me try a little moonlight madness\n","Try a little moonlight madness\n","Try a little moonlight madness\n","On you\n","\n","\n","4\n","[(0, 0.27769703), (2, 0.106872655), (3, 0.4330142), (4, 0.17747293)]\n","Girl we made it to the top\n","We went so high we couldn't stop\n","We climbed the ladder leading us nowhere\n","Two of us together\n","Building castles in the air\n","\n","We spun so fast we couldn't tell\n","The gold ring from the carousel\n","How could we know the ride would turn out bad\n","Everything we wanted\n","Was everything we had\n","\n","I miss the hungry years\n","The once upon a time\n","The lovely long ago\n","We didn't have a dime\n","Those days of me and you\n","We lost along the way\n","\n","How could I be so blind\n","Not to see the door\n","Closing on the world\n","I now hunger for\n","Looking through my tears\n","I miss the hungry years\n","\n","We shared our daydreams one by one\n","Making plans was so much fun\n","We set our goals and reached the highest star\n","The things that we were after\n","Were much better from afar\n","\n","Here we stand just me and you\n","With everything and nothing, too\n","It wasn't worth the price we had to pay\n","Honey take me home\n","Let's go back to yesterday\n","\n","I miss the hungry years\n","The once upon a time\n","The lovely long ago\n","We didn't have a dime\n","Those days of me and you\n","We lost along the way\n","\n","How could I be so blind\n","Not to see the door\n","Closing on the world\n","I know hunger for\n","Looking through my tears\n","I miss the hungry years\n","I miss the hungry years\n","3\n"]}]},{"cell_type":"code","source":["Y_val_tokenized = lyric_tokenization(Y_val)\n","Y_val_tokenized = token_filtering(Y_val_tokenized)\n","Y_val_tokenized = lemmatization(Y_val_tokenized)\n","Y_val_tokenized = remove_stop_words(Y_val_tokenized)"],"metadata":{"id":"L046RDU6v8da"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# write the validation CSV file\n","print(Y_val[0])\n","\n","with open('/content/drive/Shareddrives/CS260-Project/data/lda-val-6.csv', 'w') as valdata:\n","  writer = csv.writer(valdata, delimiter=',')\n","  writer.writerow(['artist', 'topic_id', 'lyric'])\n","  line = 0\n","  for i, artist in enumerate(X_val):\n","    curr_doc = dictionary.doc2bow(Y_val_tokenized[i])\n","    #gensim_corpus = [dictionary.doc2bow(song) for song in Y_val_tokenized]\n","    probs = lda_model[curr_doc]\n","    max_prob = -1\n","    topic_id = -1\n","    for j in range(len(probs)):\n","      idx, curr_prob = probs[j]\n","      if curr_prob > max_prob:\n","        max_prob = curr_prob\n","        topic_id = idx\n","    if line < 4:\n","      print(probs)\n","      print(Y_val[i])\n","      print(topic_id)\n","    #max_prob = max(probs)\n","    #topic_id = probs.index(max_prob)\n","    lyrics = Y_val[i]\n","    writer.writerow([artist, topic_id, lyrics])\n","    line += 1"],"metadata":{"id":"8zBswbxjulNI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669751506633,"user_tz":480,"elapsed":11747,"user":{"displayName":"Christina Chance","userId":"04108515593179486263"}},"outputId":"b1dd89d5-afb6-4007-8aa5-b1ec6d5e5aa2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["hold time feel break feel untrue convince speak voice tear try hold hurt try forgive okay play break string feel heart want feel tell real truth hurt lie worse anymore little turn dust play house ruin run leave save like chase train late late tear try hold hurt try forgive okay play break string feel heart want feel tell real truth hurt lie worse anymore little run leave save like chase train know late late play break string feel heart want feel tell real truth hurt lie worse anymore little know little hold time feel\n","[(0, 0.10326173), (2, 0.054414734), (3, 0.10834018), (4, 0.72854453)]\n","hold time feel break feel untrue convince speak voice tear try hold hurt try forgive okay play break string feel heart want feel tell real truth hurt lie worse anymore little turn dust play house ruin run leave save like chase train late late tear try hold hurt try forgive okay play break string feel heart want feel tell real truth hurt lie worse anymore little run leave save like chase train know late late play break string feel heart want feel tell real truth hurt lie worse anymore little know little hold time feel\n","4\n","[(0, 0.43901396), (2, 0.011839965), (3, 0.45010316), (4, 0.08884344)]\n","believe drop rain fall grow believe darkest night candle glow believe go astray come believe believe believe smallest prayer hear believe great hear word time hear bear baby touch leaf believe believe believe lord heaven guide sin hide believe calvary die pierce believe death rise meet heaven loud amen know believe\n","3\n","[(0, 0.11735037), (1, 0.013828977), (2, 0.06391804), (3, 0.43331915), (4, 0.36520568)]\n","sweetheart send letter goodbye secret feel better wake dream think real false emotions feel heartaches hang long blue get bluer song remember cloudy hair\n","3\n","[(0, 0.605579), (1, 0.04322426), (2, 0.24901707), (3, 0.055368118), (4, 0.024705723), (5, 0.022105781)]\n","convoy light dead ahead merchantmen trump diesels hammer oily kill grind knuckle white eye alight slam hatch deadly night cunning chicken lair hound hell devil care run silent run deep final prayer warriors secret sleep merchantman nightmare silent death lie wait run silent run deep sink final sleep chill hearts fight open ocean wonder lethal silver fish boat shiver cast millions play killer victim fool obey order rehearse lifeboat shatter hull tear black smell burn jones eye watch crosswire tube ready medal chest weeks dead like rest run silent run deep final prayer warriors secret sleep merchantman nightmare\n","0\n"]}]}]}